{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "from rembg import remove\n",
    "import imghdr\n",
    "import dlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import io\n",
    "import time\n",
    "from shutil import move\n",
    "import io\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Image Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Init for Image Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_loc = r\"C:\\Users\\parvs\\Downloads\\amber\"\n",
    "\n",
    "# Format for below: Abbreviation_full_form(init value)\n",
    "# Min_Confidence(0.9), Min_Size(0.009) (proportional to image size), Min_SharpeningStat(100), Blur_Threshold(100)\n",
    "mc, ms, mss, bt = 0.9, 0.01, 100, 100\n",
    "\n",
    "# Face Crop to dimensions x, y\n",
    "x_crop = 512\n",
    "y_crop = 512\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all images to PNG for easier reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images to .png: 100%|██████████| 114/114 [00:00<00:00, 170.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_images_to_png(source_folder):\n",
    "    if not os.path.exists(source_folder):\n",
    "        print(f\"The folder {source_folder} does not exist.\")\n",
    "        return\n",
    "\n",
    "    for filename in tqdm(os.listdir(source_folder), desc = \"Processing images to .png\"):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.bmp', '.gif', '.tiff', '.webp')):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            with Image.open(file_path) as img:\n",
    "                new_filename = os.path.splitext(filename)[0] + '.png'\n",
    "                new_file_path = os.path.join(source_folder, new_filename)\n",
    "                img.save(new_file_path, 'PNG')\n",
    "                os.remove(file_path)\n",
    "\n",
    "convert_images_to_png(folder_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Same Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 111/111 [00:03<00:00, 30.88it/s]\n"
     ]
    }
   ],
   "source": [
    "def remove_black_bars(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        cropped_image = image[y:y+h, x:x+w]\n",
    "        return cropped_image\n",
    "    return image\n",
    "\n",
    "def process_images(folder_path):\n",
    "    duplicate_images_folder = os.path.join(folder_path, \"Duplicate_Images\")\n",
    "    if not os.path.exists(duplicate_images_folder):\n",
    "        os.makedirs(duplicate_images_folder)\n",
    "    \n",
    "    image_hashes = {}\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'))]\n",
    "    for filename in tqdm(files, desc=\"Processing Images\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        image_no_black_bars = remove_black_bars(image)\n",
    "        \n",
    "        pil_image = Image.fromarray(cv2.cvtColor(image_no_black_bars, cv2.COLOR_BGR2RGB))\n",
    "        hash = str(imagehash.average_hash(pil_image))\n",
    "        \n",
    "        # Check for duplicates\n",
    "        if hash in image_hashes:\n",
    "            print(f\"Duplicate found: {filename} is a duplicate of {image_hashes[hash]}\")\n",
    "            os.rename(file_path, os.path.join(duplicate_images_folder, filename))\n",
    "        else:\n",
    "            image_hashes[hash] = filename\n",
    "\n",
    "process_images(folder_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks Images for Suitability Check (Face check, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Images: 100%|██████████| 37/37 [00:05<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "current_folder = os.getcwd()\n",
    "\n",
    "modelFile = os.path.join(current_folder, \"opencv_face_detector_uint8.pb\")\n",
    "configFile = os.path.join(current_folder, \"opencv_face_detector.pbtxt\")\n",
    "\n",
    "if os.path.exists(modelFile) != True or os.path.exists(configFile) != True:\n",
    "    print(\"ERROR: KEY FILES FOR RUNNING THIS MODEL NOT FOUND, PLEASE FIND THEM AND INSTALL THEM\")\n",
    "\n",
    "else:\n",
    "    net = cv2.dnn.readNetFromTensorflow(modelFile, configFile)\n",
    "\n",
    "    def calculate_image_sharpness(image):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate the sharpness of an image using the variance of the Laplacian\n",
    "        \"\"\"\n",
    "        \n",
    "        if image is None or image.size == 0:\n",
    "            return 0\n",
    "        return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "    def is_image_blurry(image, blur_threshold):\n",
    "        \n",
    "        \"\"\"\n",
    "        Check if an image is blurry using the variance of the Laplacian method.\n",
    "        \"\"\"\n",
    "        \n",
    "        variance_of_laplacian = cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "        return variance_of_laplacian < blur_threshold\n",
    "\n",
    "    def detect_faces_and_evaluate(image, min_confidence, min_size, min_sharpness, blur_threshold):\n",
    "        \n",
    "        \"\"\"\n",
    "        Detect faces in an image and evaluate based on size, sharpness, and blurriness.\n",
    "        \"\"\"\n",
    "        \n",
    "        if image is None or image.size == 0:\n",
    "            return []\n",
    "        if is_image_blurry(image, blur_threshold):\n",
    "            return []\n",
    "        (h, w) = image.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        faces_detected = []\n",
    "        for i in range(0, detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > min_confidence:\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                startX, startY, endX, endY = max(0, startX), max(0, startY), min(w, endX), min(h, endY)\n",
    "                if startX >= endX or startY >= endY:\n",
    "                    continue\n",
    "                face_region = image[startY:endY, startX:endX]\n",
    "                if face_region.size == 0:\n",
    "                    continue\n",
    "                face_size = (endX - startX) * (endY - startY)\n",
    "                face_sharpness = calculate_image_sharpness(face_region)\n",
    "                if face_size > min_size and face_sharpness > min_sharpness:\n",
    "                    faces_detected.append((confidence, face_size, face_sharpness))\n",
    "        return faces_detected\n",
    "\n",
    "    def remove_black_bars(image):\n",
    "        \n",
    "        \"\"\"\n",
    "        Remove black bars from an image.\n",
    "        \"\"\"\n",
    "\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 1, 255, cv2.THRESH_BINARY)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            cropped_image = image[y:y+h, x:x+w]\n",
    "            return cropped_image\n",
    "        return image\n",
    "\n",
    "    def select_best_images(\n",
    "            folder_path:str, \n",
    "            min_confidence:float = 0.9, \n",
    "            min_size:float = 0.009, \n",
    "            min_sharpness:float = 100, \n",
    "            blur_threshold:float = 100\n",
    "            ):\n",
    "        \n",
    "        selected_images_folder = os.path.join(folder_path, \"SelectedImages\")\n",
    "        if not os.path.exists(selected_images_folder):\n",
    "            os.makedirs(selected_images_folder)\n",
    "        \n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'))]\n",
    "        for filename in tqdm(files, desc=\"Evaluating Images\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            image = cv2.imread(file_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            \n",
    "            image_no_black_bars = remove_black_bars(image)\n",
    "            \n",
    "            faces_detected = detect_faces_and_evaluate(image_no_black_bars, min_confidence, min_size * image_no_black_bars.size, min_sharpness, blur_threshold)\n",
    "            if len(faces_detected) == 1:\n",
    "                move(file_path, os.path.join(selected_images_folder, filename))\n",
    "\n",
    "    select_best_images(folder_loc, mc, ms, mss, bt)\n",
    "\n",
    "    selected_folder = os.path.join(folder_loc, \"SelectedImages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanved Image Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(selected_folder) != True:\n",
    "    selected_folder = folder_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Background of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_background_from_images(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    for filename in tqdm(image_files, desc=\"Processing Images\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        with open(input_path, 'rb') as input_file:\n",
    "            input_image = input_file.read()\n",
    "\n",
    "            output_image = remove(input_image)\n",
    "\n",
    "            output_image = Image.open(io.BytesIO(output_image))\n",
    "            output_image.save(output_path)\n",
    "\n",
    "remove_background_from_images(selected_folder, os.path.join(selected_folder, \"No_BG\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop Image to Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE !!\n"
     ]
    }
   ],
   "source": [
    "failed_img = []\n",
    "face_failed = []\n",
    "\n",
    "def faceCrop(folder_dir, fName, img, imp=1, x=512, y=512):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    _, ext = os.path.splitext(fName)\n",
    "    \n",
    "    fName = fName.split(\".\")[0]\n",
    "    if ext.lower() not in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']:\n",
    "        print(f\"Unsupported image format: {ext}\")\n",
    "        failed_img.append(fName)\n",
    "        return\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    if len(faces) == 0:\n",
    "        None\n",
    "        # print(\"--> TO BE ADDED\")\n",
    "        face_failed.append(fName)\n",
    "    else:\n",
    "        importance = imp\n",
    "        areas = [face.width() * face.height() for face in faces]\n",
    "        max_area_index = np.argmax(areas)\n",
    "        fx, fy, fw, fh = faces[max_area_index].left(), faces[max_area_index].top(), faces[max_area_index].width(), faces[max_area_index].height()\n",
    "\n",
    "        # Increase the size of the bounding box to include more area around the face\n",
    "        fx = max(0, fx - fw)\n",
    "        fy = max(0, fy - fh)\n",
    "        fw = min(img.shape[1] - fx, fw * 3)\n",
    "        fh = min(img.shape[0] - fy, fh * 3)\n",
    "\n",
    "        # Crop the largest possible square within the bounding box\n",
    "        if fw > fh:\n",
    "            fx += (fw - fh) // 2\n",
    "            fw = fh\n",
    "        else:\n",
    "            fy += (fh - fw) // 2\n",
    "            fh = fw\n",
    "\n",
    "        cropped = img[fy:fy+fh, fx:fx+fw]\n",
    "\n",
    "        # Resize the cropped image to the specified dimensions\n",
    "        cropped = cv2.resize(cropped, (x, y))\n",
    "\n",
    "        cv2.imwrite(os.path.join(folder_dir, fName + '_cropped' + '.png'), cropped)\n",
    "\n",
    "def main_call(folder_path, x=512, y=512):\n",
    "    resized_folder_path = os.path.join(folder_path, \"Cropped_Images\")\n",
    "    \n",
    "    if not os.path.exists(resized_folder_path):\n",
    "        print(r\"Made /resized folder\")\n",
    "        os.makedirs(resized_folder_path)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(\"Valid Folder Location\")\n",
    "        files = os.listdir(folder_path)\n",
    "        print(\"Number of files: \", len(files))\n",
    "        itr = 0\n",
    "        for i in tqdm(files, desc=\"Processing Files\"):\n",
    "            itr += 1\n",
    "            file_loc = os.path.join(folder_path, i)\n",
    "            if os.path.isfile(file_loc) and imghdr.what(file_loc):\n",
    "                # print(\"{}/{}\".format(itr, len(files)), i, \": is image file\")\n",
    "                img = cv2.imread(file_loc)\n",
    "                if img is None:\n",
    "                    print(f\"Failed to load image: {i}:\", \"Cause: Empty !\")\n",
    "                    failed_img.append(i)\n",
    "                    continue  # Skip this file and continue with the next iteration\n",
    "                faceCrop(resized_folder_path, i, img, 1, x, y)\n",
    "            elif os.path.isdir(file_loc):\n",
    "                print(i, \": Is a folder\")\n",
    "            else:\n",
    "                print(i, \": Is not a supported Image File\")\n",
    "\n",
    "    # Clear the output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    print(\"DONE !!\")\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    if len(failed_img) != 0:\n",
    "        print(\"\\nThese images failed: \\nReason: Invalid to load: \\n\", failed_img, \"\\nCount:\", len(failed_img))\n",
    "\n",
    "    if len(face_failed) != 0:\n",
    "        print(\"\\nThese images failed: \\nReason: Face not found: \\n\", face_failed, \"\\nCount:\", len(face_failed))\n",
    "\n",
    "main_call(selected_folder, x_crop, y_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of folder along with caption files in a .txt format\n",
    "folder_loc = ''\n",
    "\n",
    "# Maximum count of words to show in \"most used words\"\n",
    "max_word_count = 100\n",
    "\n",
    "# Unique name of character\n",
    "charName = ''\n",
    "phrase_to_delete = \", \"+ charName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display most USED words in captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_words(folder_path, mc=max_word_count):  # Added mc parameter with a default value\n",
    "    \n",
    "    mc = int(mc)\n",
    "    \n",
    "    word_counts = Counter()\n",
    "    word_order = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                words = file.read().lower().split()\n",
    "                for word in words:\n",
    "                    if word not in word_order:\n",
    "                        word_order.append(word)\n",
    "                word_counts.update(words)\n",
    "    \n",
    "    common_words_df = pd.DataFrame(word_counts.most_common(mc), columns=['Word', 'Frequency'])\n",
    "    common_words_dict = dict(word_counts.most_common(mc))\n",
    "    \n",
    "    chronological_list = sorted(common_words_dict.keys(), key=lambda word: common_words_dict[word], reverse=True)\n",
    "    print(\"Common words: \", \"\\n\", common_words_df, \"\\n\", \"Common words Dictionary: \", \"\\n\", common_words_dict, \"\\n\", \"Common words List: \", \"\\n\", chronological_list, \"\\n\")\n",
    "    \n",
    "    return common_words_df, common_words_dict, chronological_list\n",
    "\n",
    "df, word_dict, word_list = get_most_common_words(folder_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the words that you want to delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to this list\n",
    "words_to_delete_list = ['photo_\\\\(medium\\\\)', '3d', 'blurry', 'blur']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update .txt files with required words (to be removed/added/updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(folder_path, words_list=words_to_delete_list, charName=charName, phrase_to_delete=phrase_to_delete):\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=\"Processing Caption Files\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if filename.endswith('.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read().lower()\n",
    "                words = content.split()\n",
    "             \n",
    "            updated_words = [word for word in words if word not in words_list]\n",
    "            \n",
    "            # Add the unique word (charName) if not already in content\n",
    "            if charName.lower() not in updated_words:\n",
    "                updated_words.append(charName.lower())\n",
    "            \n",
    "            # Use regex to find all case variations of the unique word\n",
    "            pattern = re.compile(re.escape(charName), re.IGNORECASE)\n",
    "            found_words = pattern.findall(' '.join(updated_words))\n",
    "            \n",
    "            if found_words:\n",
    "                # Remove all variations of the unique word\n",
    "                updated_content = pattern.sub('', ' '.join(updated_words)).strip()\n",
    "                \n",
    "                # Check if the unique word is already at the beginning (considering case sensitivity)\n",
    "                if not updated_content.lower().startswith(charName.lower() + ','):\n",
    "                    updated_content = charName + ', ' + updated_content\n",
    "                \n",
    "                updated_content += ', ' + charName\n",
    "                \n",
    "                # Clean up extra commas and blank entries\n",
    "                updated_content = re.sub(r',\\s*,', ', ', updated_content)  # Replace multiple commas with a single one\n",
    "                updated_content = re.sub(r'^,\\s*', '', updated_content)  # Remove leading commas\n",
    "                updated_content = re.sub(r'\\s*,\\s*$', '', updated_content)  # Remove trailing commas\n",
    "                updated_content = re.sub(r',\\s+', ', ', updated_content)  # Remove blanks between commas\n",
    "                \n",
    "                updated_content = updated_content.replace(phrase_to_delete, '')\n",
    "                \n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(updated_content)\n",
    "\n",
    "        # Convert images to PNG\n",
    "        # elif os.path.isfile(file_path):\n",
    "        #     # Temporarily construct the output file path with a .png extension\n",
    "        #     temp_output_file_path = os.path.splitext(file_path)[0] + '_temp.png'\n",
    "            \n",
    "        #     # Open and convert the image to PNG\n",
    "        #     try:\n",
    "        #         with Image.open(file_path) as img:\n",
    "        #             img.save(temp_output_file_path, 'PNG')\n",
    "                \n",
    "        #         os.remove(file_path)\n",
    "        #         os.rename(temp_output_file_path, os.path.splitext(file_path)[0] + '.png')\n",
    "        #         # print(f\"Converted and replaced {filename} with PNG.\")\n",
    "        #     except IOError:\n",
    "        #         print(f\"Skipping {filename}, not an image file.\")\n",
    "    print(\"Finished\")\n",
    "\n",
    "process_files(folder_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
